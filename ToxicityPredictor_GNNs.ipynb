{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcacac9",
   "metadata": {},
   "source": [
    "# Building Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1edbe6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit in ./.venv/lib/python3.12/site-packages (2025.9.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from rdkit) (2.3.4)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (from rdkit) (12.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rdkit\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcd7f8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.12/site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision) (2.3.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch-geometric in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from torch-geometric) (3.13.1)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch-geometric) (2025.9.0)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torch-geometric) (2.3.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./.venv/lib/python3.12/site-packages (from torch-geometric) (7.1.2)\n",
      "Requirement already satisfied: pyparsing in ./.venv/lib/python3.12/site-packages (from torch-geometric) (3.2.5)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from torch-geometric) (2.32.5)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from torch-geometric) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch-geometric) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->torch-geometric) (2025.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in ./.venv/lib/python3.12/site-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52323a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from torch_geometric.utils import from_smiles\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "CONFIG = {\n",
    "    'data_dir': './processed_tox21',\n",
    "    'hidden_channels': 128,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 64,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 0,\n",
    "    'epochs': 50,\n",
    "    'patience': 8,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Automatically detects if you have a GPU\n",
    "print(f\"Using device: {CONFIG['device']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb190d6a",
   "metadata": {},
   "source": [
    "Load data from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cfc6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6258 | Validation: 782 | Test: 783\n"
     ]
    }
   ],
   "source": [
    "def load_split(name):\n",
    "    path = os.path.join(CONFIG['data_dir'], f'tox21_{name}.pkl')\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "data_train = load_split('train')\n",
    "data_validation = load_split('validation')\n",
    "data_test = load_split('test')\n",
    "\n",
    "print(f\"Train: {len(data_train['smiles'])} | Validation: {len(data_validation['smiles'])} | Test: {len(data_test['smiles'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff422e",
   "metadata": {},
   "source": [
    "Convert SMILES to GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5af06857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(smi, labels):\n",
    "    try:\n",
    "        data = from_smiles(smi)\n",
    "        data.y = torch.tensor(labels, dtype=torch.float)\n",
    "        return data\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def make_dataset(smiles_list, label_matrix):\n",
    "    dataset = []\n",
    "    for smi, lbl in zip(smiles_list, label_matrix):\n",
    "        g = build_graph(smi, lbl)\n",
    "        if g is not None:\n",
    "            dataset.append(g)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "873e1893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs Data\n",
      "Train: 6258 | Validation: 782 | Test: 783\n"
     ]
    }
   ],
   "source": [
    "train_dataset = make_dataset(data_train['smiles'], data_train['labels'])\n",
    "validation_dataset = make_dataset(data_validation['smiles'], data_validation['labels'])\n",
    "test_dataset = make_dataset(data_test['smiles'], data_test['labels'])\n",
    "\n",
    "print(f\"Graphs Data\")\n",
    "print(f\"Train: {len(train_dataset)} | Validation: {len(validation_dataset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eac301c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = CONFIG['batch_size'], shuffle = True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size = CONFIG['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size = CONFIG['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed950470",
   "metadata": {},
   "source": [
    "Class for GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49f58632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, model_type='GCN', num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.convs = nn.ModuleList()\n",
    "        if model_type == 'GCN':\n",
    "            # Aggregates information from neighboring atoms\n",
    "            self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "            for _ in range(num_layers-1):\n",
    "                self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        elif model_type == 'GAT':\n",
    "            # Learns to weight each neighbour\n",
    "            self.convs.append(GATConv(in_channels, hidden_channels, heads=4, concat=False))\n",
    "            for _ in range(num_layers-1):\n",
    "                self.convs.append(GATConv(hidden_channels, hidden_channels, heads=4, concat=False))\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'GCN' or 'GAT'\")\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels//2, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56812b35",
   "metadata": {},
   "source": [
    "Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99df133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_bce_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes BCEWithLogitsLoss while properly ignoring NaN labels.\n",
    "    Tox21 is multi-task, and each task may be missing for some molecules.\n",
    "    \"\"\"\n",
    "    # mask indicates which label positions are valid (not NaN)\n",
    "    mask = ~torch.isnan(labels)\n",
    "\n",
    "    # replace NaNs with zeros (loss will not be computed there anyway)\n",
    "    labels = torch.where(mask, labels, torch.zeros_like(labels))\n",
    "\n",
    "    loss_f = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    loss = loss_f(logits, labels)\n",
    "\n",
    "    # apply mask so invalid labels do not contribute\n",
    "    loss = loss * mask.float()\n",
    "\n",
    "    # avoid division by zero (happens rarely if batch has all NaNs for a task)\n",
    "    if mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "    return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25e1c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in loader:\n",
    "        data.x = data.x.to(torch.float)\n",
    "        data.edge_index = data.edge_index.to(torch.long)\n",
    "        if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr.to(torch.float)\n",
    "        data.y = data.y.to(torch.float)\n",
    "\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "        if data.y.dim() == 1:\n",
    "            data.y = data.y.view(-1, logits.size(1))\n",
    "\n",
    "        loss = masked_bce_loss(logits, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2633cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data.x = data.x.to(torch.float)\n",
    "            data.edge_index = data.edge_index.to(torch.long)\n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.to(torch.float)\n",
    "            data.y = data.y.to(torch.float)\n",
    "\n",
    "            data = data.to(device)\n",
    "            \n",
    "            logits = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "            if data.y.dim() == 1:\n",
    "                data.y = data.y.view(-1, logits.size(1))\n",
    "\n",
    "            loss = masked_bce_loss(logits, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9439db8",
   "metadata": {},
   "source": [
    "Train and Evaluate GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b68009d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN\n",
      "GCN | Epoch 001 Train Loss: 0.2937 | Val Loss: 0.2509\n",
      "GCN | Epoch 002 Train Loss: 0.2548 | Val Loss: 0.2461\n",
      "GCN | Epoch 003 Train Loss: 0.2477 | Val Loss: 0.2403\n",
      "GCN | Epoch 004 Train Loss: 0.2443 | Val Loss: 0.2407\n",
      "GCN | Epoch 005 Train Loss: 0.2418 | Val Loss: 0.2341\n",
      "GCN | Epoch 006 Train Loss: 0.2398 | Val Loss: 0.2318\n",
      "GCN | Epoch 007 Train Loss: 0.2375 | Val Loss: 0.2301\n",
      "GCN | Epoch 008 Train Loss: 0.2346 | Val Loss: 0.2326\n",
      "GCN | Epoch 009 Train Loss: 0.2354 | Val Loss: 0.2312\n",
      "GCN | Epoch 010 Train Loss: 0.2338 | Val Loss: 0.2272\n",
      "GCN | Epoch 011 Train Loss: 0.2333 | Val Loss: 0.2267\n",
      "GCN | Epoch 012 Train Loss: 0.2313 | Val Loss: 0.2252\n",
      "GCN | Epoch 013 Train Loss: 0.2318 | Val Loss: 0.2252\n",
      "GCN | Epoch 014 Train Loss: 0.2298 | Val Loss: 0.2252\n",
      "GCN | Epoch 015 Train Loss: 0.2320 | Val Loss: 0.2252\n",
      "GCN | Epoch 016 Train Loss: 0.2295 | Val Loss: 0.2239\n",
      "GCN | Epoch 017 Train Loss: 0.2292 | Val Loss: 0.2235\n",
      "GCN | Epoch 018 Train Loss: 0.2291 | Val Loss: 0.2270\n",
      "GCN | Epoch 019 Train Loss: 0.2288 | Val Loss: 0.2235\n",
      "GCN | Epoch 020 Train Loss: 0.2268 | Val Loss: 0.2238\n",
      "GCN | Epoch 021 Train Loss: 0.2296 | Val Loss: 0.2270\n",
      "GCN | Epoch 022 Train Loss: 0.2280 | Val Loss: 0.2231\n",
      "GCN | Epoch 023 Train Loss: 0.2259 | Val Loss: 0.2222\n",
      "GCN | Epoch 024 Train Loss: 0.2261 | Val Loss: 0.2213\n",
      "GCN | Epoch 025 Train Loss: 0.2251 | Val Loss: 0.2235\n",
      "GCN | Epoch 026 Train Loss: 0.2256 | Val Loss: 0.2213\n",
      "GCN | Epoch 027 Train Loss: 0.2245 | Val Loss: 0.2216\n",
      "GCN | Epoch 028 Train Loss: 0.2258 | Val Loss: 0.2253\n",
      "GCN | Epoch 029 Train Loss: 0.2239 | Val Loss: 0.2280\n",
      "GCN | Epoch 030 Train Loss: 0.2243 | Val Loss: 0.2197\n",
      "GCN | Epoch 031 Train Loss: 0.2252 | Val Loss: 0.2208\n",
      "GCN | Epoch 032 Train Loss: 0.2231 | Val Loss: 0.2198\n",
      "GCN | Epoch 033 Train Loss: 0.2220 | Val Loss: 0.2184\n",
      "GCN | Epoch 034 Train Loss: 0.2224 | Val Loss: 0.2180\n",
      "GCN | Epoch 035 Train Loss: 0.2227 | Val Loss: 0.2187\n",
      "GCN | Epoch 036 Train Loss: 0.2223 | Val Loss: 0.2179\n",
      "GCN | Epoch 037 Train Loss: 0.2234 | Val Loss: 0.2199\n",
      "GCN | Epoch 038 Train Loss: 0.2222 | Val Loss: 0.2194\n",
      "GCN | Epoch 039 Train Loss: 0.2219 | Val Loss: 0.2174\n",
      "GCN | Epoch 040 Train Loss: 0.2211 | Val Loss: 0.2160\n",
      "GCN | Epoch 041 Train Loss: 0.2206 | Val Loss: 0.2186\n",
      "GCN | Epoch 042 Train Loss: 0.2207 | Val Loss: 0.2161\n",
      "GCN | Epoch 043 Train Loss: 0.2194 | Val Loss: 0.2160\n",
      "GCN | Epoch 044 Train Loss: 0.2218 | Val Loss: 0.2189\n",
      "GCN | Epoch 045 Train Loss: 0.2193 | Val Loss: 0.2140\n",
      "GCN | Epoch 046 Train Loss: 0.2197 | Val Loss: 0.2148\n",
      "GCN | Epoch 047 Train Loss: 0.2199 | Val Loss: 0.2151\n",
      "GCN | Epoch 048 Train Loss: 0.2192 | Val Loss: 0.2217\n",
      "GCN | Epoch 049 Train Loss: 0.2177 | Val Loss: 0.2139\n",
      "GCN | Epoch 050 Train Loss: 0.2165 | Val Loss: 0.2146\n",
      "GCN Test Loss: 0.2317\n",
      "Training GAT\n",
      "GAT | Epoch 001 Train Loss: 0.3377 | Val Loss: 0.2498\n",
      "GAT | Epoch 002 Train Loss: 0.2535 | Val Loss: 0.2386\n",
      "GAT | Epoch 003 Train Loss: 0.2460 | Val Loss: 0.2372\n",
      "GAT | Epoch 004 Train Loss: 0.2435 | Val Loss: 0.2371\n",
      "GAT | Epoch 005 Train Loss: 0.2431 | Val Loss: 0.2357\n",
      "GAT | Epoch 006 Train Loss: 0.2418 | Val Loss: 0.2343\n",
      "GAT | Epoch 007 Train Loss: 0.2395 | Val Loss: 0.2335\n",
      "GAT | Epoch 008 Train Loss: 0.2374 | Val Loss: 0.2374\n",
      "GAT | Epoch 009 Train Loss: 0.2361 | Val Loss: 0.2311\n",
      "GAT | Epoch 010 Train Loss: 0.2357 | Val Loss: 0.2267\n",
      "GAT | Epoch 011 Train Loss: 0.2319 | Val Loss: 0.2262\n",
      "GAT | Epoch 012 Train Loss: 0.2311 | Val Loss: 0.2286\n",
      "GAT | Epoch 013 Train Loss: 0.2308 | Val Loss: 0.2242\n",
      "GAT | Epoch 014 Train Loss: 0.2293 | Val Loss: 0.2219\n",
      "GAT | Epoch 015 Train Loss: 0.2279 | Val Loss: 0.2224\n",
      "GAT | Epoch 016 Train Loss: 0.2275 | Val Loss: 0.2205\n",
      "GAT | Epoch 017 Train Loss: 0.2279 | Val Loss: 0.2213\n",
      "GAT | Epoch 018 Train Loss: 0.2266 | Val Loss: 0.2186\n",
      "GAT | Epoch 019 Train Loss: 0.2255 | Val Loss: 0.2176\n",
      "GAT | Epoch 020 Train Loss: 0.2270 | Val Loss: 0.2182\n",
      "GAT | Epoch 021 Train Loss: 0.2247 | Val Loss: 0.2191\n",
      "GAT | Epoch 022 Train Loss: 0.2246 | Val Loss: 0.2192\n",
      "GAT | Epoch 023 Train Loss: 0.2230 | Val Loss: 0.2233\n",
      "GAT | Epoch 024 Train Loss: 0.2229 | Val Loss: 0.2153\n",
      "GAT | Epoch 025 Train Loss: 0.2225 | Val Loss: 0.2153\n",
      "GAT | Epoch 026 Train Loss: 0.2226 | Val Loss: 0.2256\n",
      "GAT | Epoch 027 Train Loss: 0.2222 | Val Loss: 0.2137\n",
      "GAT | Epoch 028 Train Loss: 0.2218 | Val Loss: 0.2133\n",
      "GAT | Epoch 029 Train Loss: 0.2214 | Val Loss: 0.2126\n",
      "GAT | Epoch 030 Train Loss: 0.2195 | Val Loss: 0.2124\n",
      "GAT | Epoch 031 Train Loss: 0.2198 | Val Loss: 0.2184\n",
      "GAT | Epoch 032 Train Loss: 0.2183 | Val Loss: 0.2131\n",
      "GAT | Epoch 033 Train Loss: 0.2187 | Val Loss: 0.2131\n",
      "GAT | Epoch 034 Train Loss: 0.2175 | Val Loss: 0.2128\n",
      "GAT | Epoch 035 Train Loss: 0.2169 | Val Loss: 0.2106\n",
      "GAT | Epoch 036 Train Loss: 0.2175 | Val Loss: 0.2124\n",
      "GAT | Epoch 037 Train Loss: 0.2162 | Val Loss: 0.2104\n",
      "GAT | Epoch 038 Train Loss: 0.2180 | Val Loss: 0.2129\n",
      "GAT | Epoch 039 Train Loss: 0.2166 | Val Loss: 0.2108\n",
      "GAT | Epoch 040 Train Loss: 0.2165 | Val Loss: 0.2091\n",
      "GAT | Epoch 041 Train Loss: 0.2156 | Val Loss: 0.2090\n",
      "GAT | Epoch 042 Train Loss: 0.2145 | Val Loss: 0.2109\n",
      "GAT | Epoch 043 Train Loss: 0.2172 | Val Loss: 0.2116\n",
      "GAT | Epoch 044 Train Loss: 0.2149 | Val Loss: 0.2106\n",
      "GAT | Epoch 045 Train Loss: 0.2143 | Val Loss: 0.2093\n",
      "GAT | Epoch 046 Train Loss: 0.2153 | Val Loss: 0.2112\n",
      "GAT | Epoch 047 Train Loss: 0.2141 | Val Loss: 0.2099\n",
      "GAT | Epoch 048 Train Loss: 0.2138 | Val Loss: 0.2105\n",
      "GAT | Epoch 049 Train Loss: 0.2117 | Val Loss: 0.2083\n",
      "GAT | Epoch 050 Train Loss: 0.2138 | Val Loss: 0.2123\n",
      "GAT Test Loss: 0.2271\n",
      "Summary of training:\n",
      "Model | Val Loss | Test Loss\n",
      "GCN  | 0.2139 | 0.2317\n",
      "GAT  | 0.2083 | 0.2271\n"
     ]
    }
   ],
   "source": [
    "sample_graph = train_dataset[0]\n",
    "in_channels = sample_graph.x.shape[1]\n",
    "out_channels = sample_graph.y.shape[0]\n",
    "results = {}\n",
    "results_rows = []\n",
    "roc_rows = []\n",
    "\n",
    "for model_type in ['GCN', 'GAT']:\n",
    "    print(f\"Training {model_type}\")\n",
    "    model = GNNModel(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=CONFIG['hidden_channels'],\n",
    "        out_channels=out_channels,\n",
    "        model_type=model_type,\n",
    "        num_layers=CONFIG['num_layers'],\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, CONFIG['epochs']+1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, CONFIG['device'])\n",
    "        val_loss = evaluate(model, val_loader, CONFIG['device'])\n",
    "\n",
    "        results_rows.append({\n",
    "            \"Model\": model_type,\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": round(train_loss, 4),\n",
    "            \"Val Loss\": round(val_loss, 4)\n",
    "        })\n",
    "\n",
    "        print(f\"{model_type} | Epoch {epoch:03d} Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"best_{model_type}.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"Early stopping {model_type}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"best_{model_type}.pt\"))\n",
    "    test_loss = evaluate(model, test_loader, CONFIG['device'])\n",
    "    print(f\"{model_type} Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    results_rows.append({\n",
    "        \"Model\": model_type,\n",
    "        \"Epoch\": \"Best\",\n",
    "        \"Train Loss\": None,\n",
    "        \"Val Loss\": round(test_loss, 4),\n",
    "    })\n",
    "\n",
    "    results[model_type] = {'val_loss': best_val_loss, 'test_loss': test_loss}\n",
    "    \n",
    "\n",
    "print(\"Summary of training:\")\n",
    "print(\"Model | Val Loss | Test Loss\")\n",
    "for model_type, res in results.items():\n",
    "    print(f\"{model_type:4s} | {res['val_loss']:.4f} | {res['test_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79480020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scrollable_df(df, height):\n",
    "    html = df.to_html()\n",
    "    display(HTML(\n",
    "        f'<div style=\"max-height:{height}px; overflow-y:auto; border:1px solid #ddd; padding:10px;\">{html}</div>'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ffb26e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:500px; overflow-y:auto; border:1px solid #ddd; padding:10px;\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Val Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2937</td>\n",
       "      <td>0.2509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2548</td>\n",
       "      <td>0.2461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2477</td>\n",
       "      <td>0.2403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GCN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2443</td>\n",
       "      <td>0.2407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GCN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2418</td>\n",
       "      <td>0.2341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GCN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2398</td>\n",
       "      <td>0.2318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GCN</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2375</td>\n",
       "      <td>0.2301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GCN</td>\n",
       "      <td>8</td>\n",
       "      <td>0.2346</td>\n",
       "      <td>0.2326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GCN</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.2312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GCN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2338</td>\n",
       "      <td>0.2272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GCN</td>\n",
       "      <td>11</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.2267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GCN</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2313</td>\n",
       "      <td>0.2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GCN</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2318</td>\n",
       "      <td>0.2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GCN</td>\n",
       "      <td>14</td>\n",
       "      <td>0.2298</td>\n",
       "      <td>0.2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GCN</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GCN</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2295</td>\n",
       "      <td>0.2239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GCN</td>\n",
       "      <td>17</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>0.2235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GCN</td>\n",
       "      <td>18</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.2270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GCN</td>\n",
       "      <td>19</td>\n",
       "      <td>0.2288</td>\n",
       "      <td>0.2235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GCN</td>\n",
       "      <td>20</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.2238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GCN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.2270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GCN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GCN</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2259</td>\n",
       "      <td>0.2222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GCN</td>\n",
       "      <td>24</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.2213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GCN</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2251</td>\n",
       "      <td>0.2235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GCN</td>\n",
       "      <td>26</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.2213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GCN</td>\n",
       "      <td>27</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.2216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GCN</td>\n",
       "      <td>28</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.2253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GCN</td>\n",
       "      <td>29</td>\n",
       "      <td>0.2239</td>\n",
       "      <td>0.2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GCN</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2243</td>\n",
       "      <td>0.2197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>GCN</td>\n",
       "      <td>31</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.2208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>GCN</td>\n",
       "      <td>32</td>\n",
       "      <td>0.2231</td>\n",
       "      <td>0.2198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GCN</td>\n",
       "      <td>33</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.2184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GCN</td>\n",
       "      <td>34</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>0.2180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GCN</td>\n",
       "      <td>35</td>\n",
       "      <td>0.2227</td>\n",
       "      <td>0.2187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>GCN</td>\n",
       "      <td>36</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>0.2179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>GCN</td>\n",
       "      <td>37</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>GCN</td>\n",
       "      <td>38</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GCN</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2219</td>\n",
       "      <td>0.2174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>GCN</td>\n",
       "      <td>40</td>\n",
       "      <td>0.2211</td>\n",
       "      <td>0.2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>GCN</td>\n",
       "      <td>41</td>\n",
       "      <td>0.2206</td>\n",
       "      <td>0.2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>GCN</td>\n",
       "      <td>42</td>\n",
       "      <td>0.2207</td>\n",
       "      <td>0.2161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>GCN</td>\n",
       "      <td>43</td>\n",
       "      <td>0.2194</td>\n",
       "      <td>0.2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>GCN</td>\n",
       "      <td>44</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.2189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>GCN</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2193</td>\n",
       "      <td>0.2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>GCN</td>\n",
       "      <td>46</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.2148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>GCN</td>\n",
       "      <td>47</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>0.2151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>GCN</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2192</td>\n",
       "      <td>0.2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>GCN</td>\n",
       "      <td>49</td>\n",
       "      <td>0.2177</td>\n",
       "      <td>0.2139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>GCN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2165</td>\n",
       "      <td>0.2146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>GCN</td>\n",
       "      <td>Best</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>GAT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3377</td>\n",
       "      <td>0.2498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>GAT</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.2386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>GAT</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2460</td>\n",
       "      <td>0.2372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>GAT</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.2371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>GAT</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.2357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>GAT</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2418</td>\n",
       "      <td>0.2343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>GAT</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.2335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GAT</td>\n",
       "      <td>8</td>\n",
       "      <td>0.2374</td>\n",
       "      <td>0.2374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>GAT</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2361</td>\n",
       "      <td>0.2311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>GAT</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2357</td>\n",
       "      <td>0.2267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>GAT</td>\n",
       "      <td>11</td>\n",
       "      <td>0.2319</td>\n",
       "      <td>0.2262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>GAT</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2311</td>\n",
       "      <td>0.2286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>GAT</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.2242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>GAT</td>\n",
       "      <td>14</td>\n",
       "      <td>0.2293</td>\n",
       "      <td>0.2219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>GAT</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2279</td>\n",
       "      <td>0.2224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>GAT</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.2205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>GAT</td>\n",
       "      <td>17</td>\n",
       "      <td>0.2279</td>\n",
       "      <td>0.2213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>GAT</td>\n",
       "      <td>18</td>\n",
       "      <td>0.2266</td>\n",
       "      <td>0.2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>GAT</td>\n",
       "      <td>19</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.2176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>GAT</td>\n",
       "      <td>20</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.2182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>GAT</td>\n",
       "      <td>21</td>\n",
       "      <td>0.2247</td>\n",
       "      <td>0.2191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>GAT</td>\n",
       "      <td>22</td>\n",
       "      <td>0.2246</td>\n",
       "      <td>0.2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>GAT</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2230</td>\n",
       "      <td>0.2233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>GAT</td>\n",
       "      <td>24</td>\n",
       "      <td>0.2229</td>\n",
       "      <td>0.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>GAT</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.2153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>GAT</td>\n",
       "      <td>26</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.2256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>GAT</td>\n",
       "      <td>27</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>GAT</td>\n",
       "      <td>28</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.2133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>GAT</td>\n",
       "      <td>29</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.2126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>GAT</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>GAT</td>\n",
       "      <td>31</td>\n",
       "      <td>0.2198</td>\n",
       "      <td>0.2184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>GAT</td>\n",
       "      <td>32</td>\n",
       "      <td>0.2183</td>\n",
       "      <td>0.2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>GAT</td>\n",
       "      <td>33</td>\n",
       "      <td>0.2187</td>\n",
       "      <td>0.2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>GAT</td>\n",
       "      <td>34</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>GAT</td>\n",
       "      <td>35</td>\n",
       "      <td>0.2169</td>\n",
       "      <td>0.2106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>GAT</td>\n",
       "      <td>36</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>GAT</td>\n",
       "      <td>37</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.2104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>GAT</td>\n",
       "      <td>38</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.2129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>GAT</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2166</td>\n",
       "      <td>0.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>GAT</td>\n",
       "      <td>40</td>\n",
       "      <td>0.2165</td>\n",
       "      <td>0.2091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>GAT</td>\n",
       "      <td>41</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.2090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>GAT</td>\n",
       "      <td>42</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>GAT</td>\n",
       "      <td>43</td>\n",
       "      <td>0.2172</td>\n",
       "      <td>0.2116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>GAT</td>\n",
       "      <td>44</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.2106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>GAT</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.2093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>GAT</td>\n",
       "      <td>46</td>\n",
       "      <td>0.2153</td>\n",
       "      <td>0.2112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>GAT</td>\n",
       "      <td>47</td>\n",
       "      <td>0.2141</td>\n",
       "      <td>0.2099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>GAT</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2138</td>\n",
       "      <td>0.2105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>GAT</td>\n",
       "      <td>49</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.2083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>GAT</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2138</td>\n",
       "      <td>0.2123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>GAT</td>\n",
       "      <td>Best</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_rows)\n",
    "display_scrollable_df(results_df, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af850dd9",
   "metadata": {},
   "source": [
    "AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d09796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_task_auc(model, loader, device):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data.x = data.x.to(torch.float)\n",
    "            data.edge_index = data.edge_index.to(torch.long)\n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.to(torch.float)\n",
    "            data.y = data.y.to(torch.float)\n",
    "\n",
    "            data = data.to(device)\n",
    "\n",
    "            logits = model(data.x, data.edge_index, data.batch)\n",
    "            labels = data.y\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits = torch.cat(all_logits, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    logits = logits.numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    aucs = []\n",
    "    for task_idx in range(labels.shape[1]):\n",
    "        valid = ~np.isnan(labels[:, task_idx])\n",
    "        if valid.sum() > 0:\n",
    "            aucs.append(roc_auc_score(labels[valid, task_idx], logits[valid, task_idx]))\n",
    "        else:\n",
    "            aucs.append(np.nan)\n",
    "\n",
    "    return aucs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8564d98e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     13\u001b[39m gat_model = GNNModel(\n\u001b[32m     14\u001b[39m     in_channels=in_channels,\n\u001b[32m     15\u001b[39m     hidden_channels=CONFIG[\u001b[33m'\u001b[39m\u001b[33mhidden_channels\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     dropout=CONFIG[\u001b[33m'\u001b[39m\u001b[33mdropout\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     20\u001b[39m ).to(CONFIG[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     21\u001b[39m gat_model.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33mbest_GAT.pt\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m gcn_auc = \u001b[43mcompute_task_auc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m gat_auc = compute_task_auc(gat_model, test_loader, CONFIG[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     26\u001b[39m roc_df = pd.DataFrame({\n\u001b[32m     27\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mTask\u001b[39m\u001b[33m'\u001b[39m: task_names,\n\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mGCN AUC-ROC\u001b[39m\u001b[33m'\u001b[39m: gcn_auc,\n\u001b[32m     29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mGAT AUC-ROC\u001b[39m\u001b[33m'\u001b[39m: gat_auc\n\u001b[32m     30\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mcompute_task_auc\u001b[39m\u001b[34m(model, loader, device)\u001b[39m\n\u001b[32m     25\u001b[39m labels = labels.numpy()\n\u001b[32m     27\u001b[39m aucs = []\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[32m     29\u001b[39m     valid = ~np.isnan(labels[:, task_idx])\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m valid.sum() > \u001b[32m0\u001b[39m:\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "task_names = data_train['labels_cols']\n",
    "\n",
    "gcn_model = GNNModel(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=CONFIG['hidden_channels'],\n",
    "    out_channels=out_channels,\n",
    "    model_type='GCN',\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(CONFIG['device'])\n",
    "gcn_model.load_state_dict(torch.load(\"best_GCN.pt\"))\n",
    "\n",
    "gat_model = GNNModel(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=CONFIG['hidden_channels'],\n",
    "    out_channels=out_channels,\n",
    "    model_type='GAT',\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(CONFIG['device'])\n",
    "gat_model.load_state_dict(torch.load(\"best_GAT.pt\"))\n",
    "\n",
    "gcn_auc = compute_task_auc(gcn_model, test_loader, CONFIG['device'])\n",
    "gat_auc = compute_task_auc(gat_model, test_loader, CONFIG['device'])\n",
    "\n",
    "roc_df = pd.DataFrame({\n",
    "    'Task': task_names,\n",
    "    'GCN AUC-ROC': gcn_auc,\n",
    "    'GAT AUC-ROC': gat_auc\n",
    "})\n",
    "\n",
    "display_scrollable_df(roc_df, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9945fa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1173, 9])\n",
      "y: torch.Size([768])\n",
      "batch: torch.Size([1173])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_loader))\n",
    "print(\"x:\", batch.x.shape)\n",
    "print(\"y:\", batch.y.shape)\n",
    "print(\"batch:\", batch.batch.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
