{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcacac9",
   "metadata": {},
   "source": [
    "# Building Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1edbe6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rdkit in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (2025.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from rdkit) (2.2.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from rdkit) (11.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install rdkit\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd7f8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (0.24.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch-geometric in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (3.11.18)\n",
      "Requirement already satisfied: fsspec in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (2025.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (2.2.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (7.1.1)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (1.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch-geometric) (3.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52323a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from torch_geometric.utils import from_smiles\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "CONFIG = {\n",
    "    'data_dir': './processed_tox21',\n",
    "    'hidden_channels': 128,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 64,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 0,\n",
    "    'epochs': 50,\n",
    "    'patience': 8,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Automatically detects if you have a GPU\n",
    "print(f\"Using device: {CONFIG['device']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb190d6a",
   "metadata": {},
   "source": [
    "Load data from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cfc6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6258 | Validation: 782 | Test: 783\n"
     ]
    }
   ],
   "source": [
    "def load_split(name):\n",
    "    path = os.path.join(CONFIG['data_dir'], f'tox21_{name}.pkl')\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "data_train = load_split('train')\n",
    "data_validation = load_split('validation')\n",
    "data_test = load_split('test')\n",
    "\n",
    "print(f\"Train: {len(data_train['smiles'])} | Validation: {len(data_validation['smiles'])} | Test: {len(data_test['smiles'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff422e",
   "metadata": {},
   "source": [
    "Convert SMILES to GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af06857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(smi, labels):\n",
    "    try:\n",
    "        data = from_smiles(smi)\n",
    "        data.y = torch.tensor(labels, dtype=torch.float)\n",
    "        return data\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def make_dataset(smiles_list, label_matrix):\n",
    "    dataset = []\n",
    "    for smi, lbl in zip(smiles_list, label_matrix):\n",
    "        g = build_graph(smi, lbl)\n",
    "        if g is not None:\n",
    "            dataset.append(g)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873e1893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs Data\n",
      "Train: 6258 | Validation: 782 | Test: 783\n"
     ]
    }
   ],
   "source": [
    "train_dataset = make_dataset(data_train['smiles'], data_train['labels'])\n",
    "validation_dataset = make_dataset(data_validation['smiles'], data_validation['labels'])\n",
    "test_dataset = make_dataset(data_test['smiles'], data_test['labels'])\n",
    "\n",
    "print(f\"Graphs Data\")\n",
    "print(f\"Train: {len(train_dataset)} | Validation: {len(validation_dataset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac301c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = CONFIG['batch_size'], shuffle = True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size = CONFIG['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size = CONFIG['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed950470",
   "metadata": {},
   "source": [
    "Class for GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f58632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, model_type='GCN', num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.convs = nn.ModuleList()\n",
    "        if model_type == 'GCN':\n",
    "            # Aggregates information from neighboring atoms\n",
    "            self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "            for _ in range(num_layers-1):\n",
    "                self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        elif model_type == 'GAT':\n",
    "            # Learns to weight each neighbour\n",
    "            self.convs.append(GATConv(in_channels, hidden_channels, heads=4, concat=False))\n",
    "            for _ in range(num_layers-1):\n",
    "                self.convs.append(GATConv(hidden_channels, hidden_channels, heads=4, concat=False))\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'GCN' or 'GAT'\")\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels//2, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56812b35",
   "metadata": {},
   "source": [
    "Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99df133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_bce_loss(logits, labels):\n",
    "    mask = ~torch.isnan(labels)\n",
    "    if mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=logits.device)\n",
    "    labels_filled = torch.where(mask, labels, torch.zeros_like(labels))\n",
    "    loss = nn.BCEWithLogitsLoss(reduction='none')(logits, labels_filled)\n",
    "    loss = loss * mask.float()\n",
    "    return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e1c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in loader:\n",
    "        data.x = data.x.to(torch.float)\n",
    "        data.edge_index = data.edge_index.to(torch.long)\n",
    "        if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr.to(torch.float)\n",
    "        data.y = data.y.to(torch.float)\n",
    "\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "        if data.y.dim() == 1:\n",
    "            data.y = data.y.view(-1, logits.size(1))\n",
    "\n",
    "        loss = masked_bce_loss(logits, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2633cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data.x = data.x.to(torch.float)\n",
    "            data.edge_index = data.edge_index.to(torch.long)\n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.to(torch.float)\n",
    "            data.y = data.y.to(torch.float)\n",
    "\n",
    "            data = data.to(device)\n",
    "            \n",
    "            logits = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "            if data.y.dim() == 1:\n",
    "                data.y = data.y.view(-1, logits.size(1))\n",
    "\n",
    "            loss = masked_bce_loss(logits, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9439db8",
   "metadata": {},
   "source": [
    "Train and Evaluate GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d09796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOX21_TASKS = [\n",
    "    \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\",\n",
    "    \"NR-ER\", \"NR-ER-LBD\", \"NR-PPAR-gamma\", \"SR-ARE\",\n",
    "    \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n",
    "]\n",
    "\n",
    "\n",
    "def compute_roc_auc_per_task(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader):\n",
    "            # Enforce correct tensor types\n",
    "            data.x = data.x.to(torch.float)\n",
    "            data.edge_index = data.edge_index.to(torch.long)\n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.to(torch.float)\n",
    "            data.y = data.y.to(torch.float)\n",
    "            data = data.to(device)\n",
    "\n",
    "            logits = model(data.x, data.edge_index, data.batch)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            y_true = data.y.cpu().numpy()\n",
    "\n",
    "            # Ensure consistent shape\n",
    "            if y_true.ndim == 1:\n",
    "                y_true = y_true.reshape(-1, 1)\n",
    "\n",
    "            if probs.shape != y_true.shape:\n",
    "                print(f\"[⚠️ Batch {i}] Shape mismatch: preds {probs.shape}, labels {y_true.shape} — skipping\")\n",
    "                continue\n",
    "\n",
    "            all_preds.append(probs)\n",
    "            all_labels.append(y_true)\n",
    "\n",
    "    if len(all_preds) == 0:\n",
    "        print(\"⚠️ No valid batches were found — returning NaN AUCs.\")\n",
    "        return {t: np.nan for t in TOX21_TASKS} | {\"Mean\": np.nan, \"Std\": np.nan}\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    aucs = {}\n",
    "    for i, task_name in enumerate(TOX21_TASKS):\n",
    "        y_true = all_labels[:, i]\n",
    "        y_pred = all_preds[:, i]\n",
    "        mask = ~np.isnan(y_true)\n",
    "        if np.sum(mask) > 0 and len(np.unique(y_true[mask])) > 1:\n",
    "            aucs[task_name] = roc_auc_score(y_true[mask], y_pred[mask])\n",
    "        else:\n",
    "            aucs[task_name] = np.nan\n",
    "\n",
    "    valid_aucs = [v for v in aucs.values() if not np.isnan(v)]\n",
    "    aucs[\"Mean\"] = np.mean(valid_aucs)\n",
    "    aucs[\"Std\"] = np.std(valid_aucs)\n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b68009d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN\n",
      "GCN | Epoch 001 Train Loss: 0.3046 | Val Loss: 0.2499\n",
      "GCN | Epoch 002 Train Loss: 0.2555 | Val Loss: 0.2443\n",
      "GCN | Epoch 003 Train Loss: 0.2469 | Val Loss: 0.2387\n",
      "GCN | Epoch 004 Train Loss: 0.2449 | Val Loss: 0.2370\n",
      "GCN | Epoch 005 Train Loss: 0.2415 | Val Loss: 0.2324\n",
      "GCN | Epoch 006 Train Loss: 0.2377 | Val Loss: 0.2440\n",
      "GCN | Epoch 007 Train Loss: 0.2365 | Val Loss: 0.2298\n",
      "GCN | Epoch 008 Train Loss: 0.2354 | Val Loss: 0.2286\n",
      "GCN | Epoch 009 Train Loss: 0.2356 | Val Loss: 0.2313\n",
      "GCN | Epoch 010 Train Loss: 0.2331 | Val Loss: 0.2277\n",
      "GCN | Epoch 011 Train Loss: 0.2320 | Val Loss: 0.2290\n",
      "GCN | Epoch 012 Train Loss: 0.2320 | Val Loss: 0.2274\n",
      "GCN | Epoch 013 Train Loss: 0.2307 | Val Loss: 0.2252\n",
      "GCN | Epoch 014 Train Loss: 0.2296 | Val Loss: 0.2245\n",
      "GCN | Epoch 015 Train Loss: 0.2312 | Val Loss: 0.2252\n",
      "GCN | Epoch 016 Train Loss: 0.2321 | Val Loss: 0.2276\n",
      "GCN | Epoch 017 Train Loss: 0.2311 | Val Loss: 0.2257\n",
      "GCN | Epoch 018 Train Loss: 0.2295 | Val Loss: 0.2297\n",
      "GCN | Epoch 019 Train Loss: 0.2282 | Val Loss: 0.2288\n",
      "GCN | Epoch 020 Train Loss: 0.2314 | Val Loss: 0.2254\n",
      "GCN | Epoch 021 Train Loss: 0.2279 | Val Loss: 0.2268\n",
      "GCN | Epoch 022 Train Loss: 0.2283 | Val Loss: 0.2243\n",
      "GCN | Epoch 023 Train Loss: 0.2268 | Val Loss: 0.2234\n",
      "GCN | Epoch 024 Train Loss: 0.2272 | Val Loss: 0.2220\n",
      "GCN | Epoch 025 Train Loss: 0.2277 | Val Loss: 0.2214\n",
      "GCN | Epoch 026 Train Loss: 0.2269 | Val Loss: 0.2261\n",
      "GCN | Epoch 027 Train Loss: 0.2262 | Val Loss: 0.2273\n",
      "GCN | Epoch 028 Train Loss: 0.2262 | Val Loss: 0.2221\n",
      "GCN | Epoch 029 Train Loss: 0.2260 | Val Loss: 0.2243\n",
      "GCN | Epoch 030 Train Loss: 0.2254 | Val Loss: 0.2203\n",
      "GCN | Epoch 031 Train Loss: 0.2247 | Val Loss: 0.2189\n",
      "GCN | Epoch 032 Train Loss: 0.2242 | Val Loss: 0.2206\n",
      "GCN | Epoch 033 Train Loss: 0.2259 | Val Loss: 0.2200\n",
      "GCN | Epoch 034 Train Loss: 0.2241 | Val Loss: 0.2191\n",
      "GCN | Epoch 035 Train Loss: 0.2228 | Val Loss: 0.2206\n",
      "GCN | Epoch 036 Train Loss: 0.2230 | Val Loss: 0.2196\n",
      "GCN | Epoch 037 Train Loss: 0.2240 | Val Loss: 0.2209\n",
      "GCN | Epoch 038 Train Loss: 0.2244 | Val Loss: 0.2187\n",
      "GCN | Epoch 039 Train Loss: 0.2232 | Val Loss: 0.2224\n",
      "GCN | Epoch 040 Train Loss: 0.2241 | Val Loss: 0.2168\n",
      "GCN | Epoch 041 Train Loss: 0.2223 | Val Loss: 0.2165\n",
      "GCN | Epoch 042 Train Loss: 0.2217 | Val Loss: 0.2171\n",
      "GCN | Epoch 043 Train Loss: 0.2213 | Val Loss: 0.2166\n",
      "GCN | Epoch 044 Train Loss: 0.2208 | Val Loss: 0.2170\n",
      "GCN | Epoch 045 Train Loss: 0.2223 | Val Loss: 0.2167\n",
      "GCN | Epoch 046 Train Loss: 0.2209 | Val Loss: 0.2157\n",
      "GCN | Epoch 047 Train Loss: 0.2210 | Val Loss: 0.2149\n",
      "GCN | Epoch 048 Train Loss: 0.2193 | Val Loss: 0.2170\n",
      "GCN | Epoch 049 Train Loss: 0.2209 | Val Loss: 0.2215\n",
      "GCN | Epoch 050 Train Loss: 0.2194 | Val Loss: 0.2152\n",
      "GCN Test Loss: 0.2323\n",
      "[⚠️ Batch 0] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 1] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 2] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 3] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 4] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 5] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 6] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 7] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 8] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 9] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 10] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 11] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 12] Shape mismatch: preds (15, 12), labels (180, 1) — skipping\n",
      "⚠️ No valid batches were found — returning NaN AUCs.\n",
      "Training GAT\n",
      "GAT | Epoch 001 Train Loss: 0.3394 | Val Loss: 0.2498\n",
      "GAT | Epoch 002 Train Loss: 0.2520 | Val Loss: 0.2445\n",
      "GAT | Epoch 003 Train Loss: 0.2458 | Val Loss: 0.2379\n",
      "GAT | Epoch 004 Train Loss: 0.2432 | Val Loss: 0.2368\n",
      "GAT | Epoch 005 Train Loss: 0.2422 | Val Loss: 0.2358\n",
      "GAT | Epoch 006 Train Loss: 0.2411 | Val Loss: 0.2409\n",
      "GAT | Epoch 007 Train Loss: 0.2393 | Val Loss: 0.2351\n",
      "GAT | Epoch 008 Train Loss: 0.2389 | Val Loss: 0.2337\n",
      "GAT | Epoch 009 Train Loss: 0.2370 | Val Loss: 0.2325\n",
      "GAT | Epoch 010 Train Loss: 0.2332 | Val Loss: 0.2281\n",
      "GAT | Epoch 011 Train Loss: 0.2330 | Val Loss: 0.2261\n",
      "GAT | Epoch 012 Train Loss: 0.2314 | Val Loss: 0.2264\n",
      "GAT | Epoch 013 Train Loss: 0.2308 | Val Loss: 0.2257\n",
      "GAT | Epoch 014 Train Loss: 0.2301 | Val Loss: 0.2266\n",
      "GAT | Epoch 015 Train Loss: 0.2288 | Val Loss: 0.2226\n",
      "GAT | Epoch 016 Train Loss: 0.2300 | Val Loss: 0.2226\n",
      "GAT | Epoch 017 Train Loss: 0.2271 | Val Loss: 0.2246\n",
      "GAT | Epoch 018 Train Loss: 0.2262 | Val Loss: 0.2215\n",
      "GAT | Epoch 019 Train Loss: 0.2267 | Val Loss: 0.2220\n",
      "GAT | Epoch 020 Train Loss: 0.2262 | Val Loss: 0.2194\n",
      "GAT | Epoch 021 Train Loss: 0.2279 | Val Loss: 0.2205\n",
      "GAT | Epoch 022 Train Loss: 0.2252 | Val Loss: 0.2202\n",
      "GAT | Epoch 023 Train Loss: 0.2247 | Val Loss: 0.2192\n",
      "GAT | Epoch 024 Train Loss: 0.2250 | Val Loss: 0.2188\n",
      "GAT | Epoch 025 Train Loss: 0.2234 | Val Loss: 0.2263\n",
      "GAT | Epoch 026 Train Loss: 0.2233 | Val Loss: 0.2186\n",
      "GAT | Epoch 027 Train Loss: 0.2221 | Val Loss: 0.2173\n",
      "GAT | Epoch 028 Train Loss: 0.2229 | Val Loss: 0.2184\n",
      "GAT | Epoch 029 Train Loss: 0.2229 | Val Loss: 0.2174\n",
      "GAT | Epoch 030 Train Loss: 0.2213 | Val Loss: 0.2169\n",
      "GAT | Epoch 031 Train Loss: 0.2217 | Val Loss: 0.2170\n",
      "GAT | Epoch 032 Train Loss: 0.2224 | Val Loss: 0.2181\n",
      "GAT | Epoch 033 Train Loss: 0.2213 | Val Loss: 0.2183\n",
      "GAT | Epoch 034 Train Loss: 0.2218 | Val Loss: 0.2173\n",
      "GAT | Epoch 035 Train Loss: 0.2207 | Val Loss: 0.2166\n",
      "GAT | Epoch 036 Train Loss: 0.2196 | Val Loss: 0.2151\n",
      "GAT | Epoch 037 Train Loss: 0.2196 | Val Loss: 0.2158\n",
      "GAT | Epoch 038 Train Loss: 0.2199 | Val Loss: 0.2157\n",
      "GAT | Epoch 039 Train Loss: 0.2197 | Val Loss: 0.2164\n",
      "GAT | Epoch 040 Train Loss: 0.2194 | Val Loss: 0.2139\n",
      "GAT | Epoch 041 Train Loss: 0.2199 | Val Loss: 0.2157\n",
      "GAT | Epoch 042 Train Loss: 0.2197 | Val Loss: 0.2188\n",
      "GAT | Epoch 043 Train Loss: 0.2199 | Val Loss: 0.2180\n",
      "GAT | Epoch 044 Train Loss: 0.2182 | Val Loss: 0.2144\n",
      "GAT | Epoch 045 Train Loss: 0.2180 | Val Loss: 0.2163\n",
      "GAT | Epoch 046 Train Loss: 0.2178 | Val Loss: 0.2151\n",
      "GAT | Epoch 047 Train Loss: 0.2179 | Val Loss: 0.2143\n",
      "GAT | Epoch 048 Train Loss: 0.2177 | Val Loss: 0.2145\n",
      "Early stopping GAT\n",
      "GAT Test Loss: 0.2305\n",
      "[⚠️ Batch 0] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 1] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 2] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 3] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 4] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 5] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 6] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 7] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 8] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 9] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 10] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 11] Shape mismatch: preds (64, 12), labels (768, 1) — skipping\n",
      "[⚠️ Batch 12] Shape mismatch: preds (15, 12), labels (180, 1) — skipping\n",
      "⚠️ No valid batches were found — returning NaN AUCs.\n",
      "Summary of training:\n",
      "Model | Val Loss | Test Loss\n",
      "GCN  | 0.2149 | 0.2323\n",
      "GAT  | 0.2139 | 0.2305\n"
     ]
    }
   ],
   "source": [
    "sample_graph = train_dataset[0]\n",
    "in_channels = sample_graph.x.shape[1]\n",
    "out_channels = sample_graph.y.shape[0]\n",
    "results = {}\n",
    "results_rows = []\n",
    "roc_rows = []\n",
    "\n",
    "for model_type in ['GCN', 'GAT']:\n",
    "    print(f\"Training {model_type}\")\n",
    "    model = GNNModel(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=CONFIG['hidden_channels'],\n",
    "        out_channels=out_channels,\n",
    "        model_type=model_type,\n",
    "        num_layers=CONFIG['num_layers'],\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, CONFIG['epochs']+1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, CONFIG['device'])\n",
    "        val_loss = evaluate(model, val_loader, CONFIG['device'])\n",
    "\n",
    "        results_rows.append({\n",
    "            \"Model\": model_type,\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": round(train_loss, 4),\n",
    "            \"Val Loss\": round(val_loss, 4)\n",
    "        })\n",
    "\n",
    "        print(f\"{model_type} | Epoch {epoch:03d} Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"best_{model_type}.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"Early stopping {model_type}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"best_{model_type}.pt\"))\n",
    "    test_loss = evaluate(model, test_loader, CONFIG['device'])\n",
    "    print(f\"{model_type} Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    results_rows.append({\n",
    "        \"Model\": model_type,\n",
    "        \"Epoch\": \"Best\",\n",
    "        \"Train Loss\": None,\n",
    "        \"Val Loss\": round(test_loss, 4),\n",
    "    })\n",
    "\n",
    "    results[model_type] = {'val_loss': best_val_loss, 'test_loss': test_loss}\n",
    "\n",
    "    roc_dict = compute_roc_auc_per_task(model, test_loader, CONFIG['device'])\n",
    "    roc_dict[\"Model\"] = model_type\n",
    "    roc_rows.append(roc_dict)\n",
    "\n",
    "print(\"Summary of training:\")\n",
    "print(\"Model | Val Loss | Test Loss\")\n",
    "for model_type, res in results.items():\n",
    "    print(f\"{model_type:4s} | {res['val_loss']:.4f} | {res['test_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79480020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scrollable_df(df, height):\n",
    "    html = df.to_html()\n",
    "    display(HTML(\n",
    "        f'<div style=\"max-height:{height}px; overflow-y:auto; border:1px solid #ddd; padding:10px;\">{html}</div>'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ffb26e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:500px; overflow-y:auto; border:1px solid #ddd; padding:10px;\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Val Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.2499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2555</td>\n",
       "      <td>0.2443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2469</td>\n",
       "      <td>0.2387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GCN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2449</td>\n",
       "      <td>0.2370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GCN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.2324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GCN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2377</td>\n",
       "      <td>0.2440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GCN</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2365</td>\n",
       "      <td>0.2298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GCN</td>\n",
       "      <td>8</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.2286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GCN</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>0.2313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GCN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2331</td>\n",
       "      <td>0.2277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GCN</td>\n",
       "      <td>11</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.2290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GCN</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.2274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GCN</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2307</td>\n",
       "      <td>0.2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GCN</td>\n",
       "      <td>14</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.2245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GCN</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2312</td>\n",
       "      <td>0.2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GCN</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2321</td>\n",
       "      <td>0.2276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GCN</td>\n",
       "      <td>17</td>\n",
       "      <td>0.2311</td>\n",
       "      <td>0.2257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GCN</td>\n",
       "      <td>18</td>\n",
       "      <td>0.2295</td>\n",
       "      <td>0.2297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GCN</td>\n",
       "      <td>19</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>0.2288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GCN</td>\n",
       "      <td>20</td>\n",
       "      <td>0.2314</td>\n",
       "      <td>0.2254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GCN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.2279</td>\n",
       "      <td>0.2268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GCN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.2283</td>\n",
       "      <td>0.2243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GCN</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GCN</td>\n",
       "      <td>24</td>\n",
       "      <td>0.2272</td>\n",
       "      <td>0.2220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GCN</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>0.2214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GCN</td>\n",
       "      <td>26</td>\n",
       "      <td>0.2269</td>\n",
       "      <td>0.2261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GCN</td>\n",
       "      <td>27</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.2273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GCN</td>\n",
       "      <td>28</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.2221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GCN</td>\n",
       "      <td>29</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.2243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GCN</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.2203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>GCN</td>\n",
       "      <td>31</td>\n",
       "      <td>0.2247</td>\n",
       "      <td>0.2189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>GCN</td>\n",
       "      <td>32</td>\n",
       "      <td>0.2242</td>\n",
       "      <td>0.2206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GCN</td>\n",
       "      <td>33</td>\n",
       "      <td>0.2259</td>\n",
       "      <td>0.2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GCN</td>\n",
       "      <td>34</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>0.2191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GCN</td>\n",
       "      <td>35</td>\n",
       "      <td>0.2228</td>\n",
       "      <td>0.2206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>GCN</td>\n",
       "      <td>36</td>\n",
       "      <td>0.2230</td>\n",
       "      <td>0.2196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>GCN</td>\n",
       "      <td>37</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.2209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>GCN</td>\n",
       "      <td>38</td>\n",
       "      <td>0.2244</td>\n",
       "      <td>0.2187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GCN</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2232</td>\n",
       "      <td>0.2224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>GCN</td>\n",
       "      <td>40</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>0.2168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>GCN</td>\n",
       "      <td>41</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>0.2165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>GCN</td>\n",
       "      <td>42</td>\n",
       "      <td>0.2217</td>\n",
       "      <td>0.2171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>GCN</td>\n",
       "      <td>43</td>\n",
       "      <td>0.2213</td>\n",
       "      <td>0.2166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>GCN</td>\n",
       "      <td>44</td>\n",
       "      <td>0.2208</td>\n",
       "      <td>0.2170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>GCN</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>0.2167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>GCN</td>\n",
       "      <td>46</td>\n",
       "      <td>0.2209</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>GCN</td>\n",
       "      <td>47</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.2149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>GCN</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2193</td>\n",
       "      <td>0.2170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>GCN</td>\n",
       "      <td>49</td>\n",
       "      <td>0.2209</td>\n",
       "      <td>0.2215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>GCN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2194</td>\n",
       "      <td>0.2152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>GCN</td>\n",
       "      <td>Best</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>GAT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>0.2498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>GAT</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.2445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>GAT</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.2379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>GAT</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.2368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>GAT</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2422</td>\n",
       "      <td>0.2358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>GAT</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2411</td>\n",
       "      <td>0.2409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>GAT</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2393</td>\n",
       "      <td>0.2351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GAT</td>\n",
       "      <td>8</td>\n",
       "      <td>0.2389</td>\n",
       "      <td>0.2337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>GAT</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.2325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>GAT</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2332</td>\n",
       "      <td>0.2281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>GAT</td>\n",
       "      <td>11</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>0.2261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>GAT</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2314</td>\n",
       "      <td>0.2264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>GAT</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.2257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>GAT</td>\n",
       "      <td>14</td>\n",
       "      <td>0.2301</td>\n",
       "      <td>0.2266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>GAT</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2288</td>\n",
       "      <td>0.2226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>GAT</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.2226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>GAT</td>\n",
       "      <td>17</td>\n",
       "      <td>0.2271</td>\n",
       "      <td>0.2246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>GAT</td>\n",
       "      <td>18</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.2215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>GAT</td>\n",
       "      <td>19</td>\n",
       "      <td>0.2267</td>\n",
       "      <td>0.2220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>GAT</td>\n",
       "      <td>20</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.2194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>GAT</td>\n",
       "      <td>21</td>\n",
       "      <td>0.2279</td>\n",
       "      <td>0.2205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>GAT</td>\n",
       "      <td>22</td>\n",
       "      <td>0.2252</td>\n",
       "      <td>0.2202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>GAT</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2247</td>\n",
       "      <td>0.2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>GAT</td>\n",
       "      <td>24</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.2188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>GAT</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>GAT</td>\n",
       "      <td>26</td>\n",
       "      <td>0.2233</td>\n",
       "      <td>0.2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>GAT</td>\n",
       "      <td>27</td>\n",
       "      <td>0.2221</td>\n",
       "      <td>0.2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>GAT</td>\n",
       "      <td>28</td>\n",
       "      <td>0.2229</td>\n",
       "      <td>0.2184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>GAT</td>\n",
       "      <td>29</td>\n",
       "      <td>0.2229</td>\n",
       "      <td>0.2174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>GAT</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2213</td>\n",
       "      <td>0.2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>GAT</td>\n",
       "      <td>31</td>\n",
       "      <td>0.2217</td>\n",
       "      <td>0.2170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>GAT</td>\n",
       "      <td>32</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>0.2181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>GAT</td>\n",
       "      <td>33</td>\n",
       "      <td>0.2213</td>\n",
       "      <td>0.2183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>GAT</td>\n",
       "      <td>34</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>GAT</td>\n",
       "      <td>35</td>\n",
       "      <td>0.2207</td>\n",
       "      <td>0.2166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>GAT</td>\n",
       "      <td>36</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.2151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>GAT</td>\n",
       "      <td>37</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>GAT</td>\n",
       "      <td>38</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>GAT</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.2164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>GAT</td>\n",
       "      <td>40</td>\n",
       "      <td>0.2194</td>\n",
       "      <td>0.2139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>GAT</td>\n",
       "      <td>41</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>GAT</td>\n",
       "      <td>42</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.2188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>GAT</td>\n",
       "      <td>43</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>0.2180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>GAT</td>\n",
       "      <td>44</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>0.2144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>GAT</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.2163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>GAT</td>\n",
       "      <td>46</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>0.2151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>GAT</td>\n",
       "      <td>47</td>\n",
       "      <td>0.2179</td>\n",
       "      <td>0.2143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>GAT</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2177</td>\n",
       "      <td>0.2145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>GAT</td>\n",
       "      <td>Best</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_rows)\n",
    "display_scrollable_df(results_df, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8564d98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:400px; overflow-y:auto; border:1px solid #ddd; padding:10px;\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NR-AR</th>\n",
       "      <th>NR-AR-LBD</th>\n",
       "      <th>NR-AhR</th>\n",
       "      <th>NR-Aromatase</th>\n",
       "      <th>NR-ER</th>\n",
       "      <th>NR-ER-LBD</th>\n",
       "      <th>NR-PPAR-gamma</th>\n",
       "      <th>SR-ARE</th>\n",
       "      <th>SR-ATAD5</th>\n",
       "      <th>SR-HSE</th>\n",
       "      <th>SR-MMP</th>\n",
       "      <th>SR-p53</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GCN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roc_df = pd.DataFrame(roc_rows).set_index(\"Model\")\n",
    "display_scrollable_df(roc_df.round(4), height=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
