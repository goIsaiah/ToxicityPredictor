{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcacac9",
   "metadata": {},
   "source": [
    "# Building Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edbe6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rdkit in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (2025.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from rdkit) (2.2.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from rdkit) (11.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install rdkit\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd7f8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (0.24.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch-geometric in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (3.11.18)\n",
      "Requirement already satisfied: fsspec in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (2025.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (2.2.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (7.1.1)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from torch-geometric) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch-geometric) (1.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch-geometric) (3.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch-geometric) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\goisa\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio\n",
    "%pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52323a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goisa\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from torch_geometric.utils import from_smiles\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "CONFIG = {\n",
    "    'data_dir': './processed_tox21',\n",
    "    'hidden_channels': 128,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 64,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 0,\n",
    "    'epochs': 50,\n",
    "    'patience': 8,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Automatically detects if you have a GPU\n",
    "print(f\"Using device: {CONFIG['device']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb190d6a",
   "metadata": {},
   "source": [
    "Load data from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfc6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6258 | Validation: 782 | Test: 783\n"
     ]
    }
   ],
   "source": [
    "def load_split(name):\n",
    "    path = os.path.join(CONFIG['data_dir'], f'tox21_{name}.pkl')\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "data_train = load_split('train')\n",
    "data_validation = load_split('validation')\n",
    "data_test = load_split('test')\n",
    "\n",
    "print(f\"Train: {len(data_train['smiles'])} | Validation: {len(data_validation['smiles'])} | Test: {len(data_test['smiles'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff422e",
   "metadata": {},
   "source": [
    "Convert SMILES to GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af06857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(smi, labels):\n",
    "    try:\n",
    "        data = from_smiles(smi)\n",
    "        labels = np.array(labels, dtype=float).reshape(-1)\n",
    "        data.y = torch.tensor(labels, dtype=torch.float).unsqueeze(0)\n",
    "        return data\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def make_dataset(smiles_list, label_matrix):\n",
    "    dataset = []\n",
    "    for smi, lbl in zip(smiles_list, label_matrix):\n",
    "        g = build_graph(smi, lbl)\n",
    "        if g is not None:\n",
    "            dataset.append(g)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873e1893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs Data\n",
      "Train: 6258 | Validation: 782 | Test: 783\n"
     ]
    }
   ],
   "source": [
    "train_dataset = make_dataset(data_train['smiles'], data_train['labels'])\n",
    "validation_dataset = make_dataset(data_validation['smiles'], data_validation['labels'])\n",
    "test_dataset = make_dataset(data_test['smiles'], data_test['labels'])\n",
    "\n",
    "print(f\"Graphs Data\")\n",
    "print(f\"Train: {len(train_dataset)} | Validation: {len(validation_dataset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac301c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = CONFIG['batch_size'], shuffle = True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size = CONFIG['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size = CONFIG['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "087e6040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single graph -- x: torch.Size([14, 9]) y: torch.Size([1, 12]) num_nodes: 14\n",
      "batch.x: torch.Size([1005, 9])\n",
      "batch.batch: torch.Size([1005]) max batch index: 63\n",
      "batch.y: torch.Size([64, 12])\n"
     ]
    }
   ],
   "source": [
    "# After rebuilding the dataset and re-creating train_dataset, etc.\n",
    "sample = train_dataset[0]\n",
    "print(\"single graph -- x:\", sample.x.shape, \"y:\", sample.y.shape, \"num_nodes:\", sample.num_nodes)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"batch.x:\", batch.x.shape)\n",
    "print(\"batch.batch:\", batch.batch.shape, \"max batch index:\", batch.batch.max().item())\n",
    "print(\"batch.y:\", batch.y.shape)   # should be (batch_size, num_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed950470",
   "metadata": {},
   "source": [
    "Class for GNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f58632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, model_type='GCN', num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.convs = nn.ModuleList()\n",
    "        if model_type == 'GCN':\n",
    "            # Aggregates information from neighboring atoms\n",
    "            self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "            for _ in range(num_layers-1):\n",
    "                self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        elif model_type == 'GAT':\n",
    "            # Learns to weight each neighbour\n",
    "            self.convs.append(GATConv(in_channels, hidden_channels, heads=4, concat=False))\n",
    "            for _ in range(num_layers-1):\n",
    "                self.convs.append(GATConv(hidden_channels, hidden_channels, heads=4, concat=False))\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'GCN' or 'GAT'\")\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels//2, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56812b35",
   "metadata": {},
   "source": [
    "Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99df133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_bce_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    logits: (batch_size, num_tasks)\n",
    "    labels: (batch_size, num_tasks) some NaNs\n",
    "    \"\"\"\n",
    "    # mask indicates which label positions are valid (not NaN)\n",
    "    mask = ~torch.isnan(labels)\n",
    "\n",
    "    if mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=logits.device)\n",
    "    \n",
    "    valid_logits = logits[mask]\n",
    "    valid_labels = labels[mask]\n",
    "    loss = nn.BCEWithLogitsLoss()(valid_logits, valid_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e1c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in loader:\n",
    "        data.x = data.x.to(torch.float)\n",
    "        data.edge_index = data.edge_index.to(torch.long)\n",
    "        if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr.to(torch.float)\n",
    "        data.y = data.y.to(torch.float)\n",
    "\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        loss = masked_bce_loss(logits, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2633cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data.x = data.x.to(torch.float)\n",
    "            data.edge_index = data.edge_index.to(torch.long)\n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.to(torch.float)\n",
    "            data.y = data.y.to(torch.float)\n",
    "\n",
    "            data = data.to(device)\n",
    "            \n",
    "            logits = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "            if data.y.dim() == 1:\n",
    "                data.y = data.y.view(-1, logits.size(1))\n",
    "\n",
    "            loss = masked_bce_loss(logits, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9439db8",
   "metadata": {},
   "source": [
    "Train and Evaluate GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b68009d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN\n",
      "GCN | Epoch 001 Train Loss: 0.3078 | Val Loss: 0.2517\n",
      "GCN | Epoch 002 Train Loss: 0.2575 | Val Loss: 0.2442\n",
      "GCN | Epoch 003 Train Loss: 0.2483 | Val Loss: 0.2407\n",
      "GCN | Epoch 004 Train Loss: 0.2445 | Val Loss: 0.2358\n",
      "GCN | Epoch 005 Train Loss: 0.2423 | Val Loss: 0.2380\n",
      "GCN | Epoch 006 Train Loss: 0.2399 | Val Loss: 0.2331\n",
      "GCN | Epoch 007 Train Loss: 0.2380 | Val Loss: 0.2325\n",
      "GCN | Epoch 008 Train Loss: 0.2368 | Val Loss: 0.2290\n",
      "GCN | Epoch 009 Train Loss: 0.2354 | Val Loss: 0.2292\n",
      "GCN | Epoch 010 Train Loss: 0.2356 | Val Loss: 0.2322\n",
      "GCN | Epoch 011 Train Loss: 0.2338 | Val Loss: 0.2270\n",
      "GCN | Epoch 012 Train Loss: 0.2316 | Val Loss: 0.2264\n",
      "GCN | Epoch 013 Train Loss: 0.2315 | Val Loss: 0.2264\n",
      "GCN | Epoch 014 Train Loss: 0.2312 | Val Loss: 0.2266\n",
      "GCN | Epoch 015 Train Loss: 0.2316 | Val Loss: 0.2258\n",
      "GCN | Epoch 016 Train Loss: 0.2312 | Val Loss: 0.2277\n",
      "GCN | Epoch 017 Train Loss: 0.2294 | Val Loss: 0.2275\n",
      "GCN | Epoch 018 Train Loss: 0.2303 | Val Loss: 0.2260\n",
      "GCN | Epoch 019 Train Loss: 0.2298 | Val Loss: 0.2251\n",
      "GCN | Epoch 020 Train Loss: 0.2292 | Val Loss: 0.2254\n",
      "GCN | Epoch 021 Train Loss: 0.2289 | Val Loss: 0.2257\n",
      "GCN | Epoch 022 Train Loss: 0.2289 | Val Loss: 0.2295\n",
      "GCN | Epoch 023 Train Loss: 0.2285 | Val Loss: 0.2255\n",
      "GCN | Epoch 024 Train Loss: 0.2280 | Val Loss: 0.2246\n",
      "GCN | Epoch 025 Train Loss: 0.2277 | Val Loss: 0.2282\n",
      "GCN | Epoch 026 Train Loss: 0.2272 | Val Loss: 0.2227\n",
      "GCN | Epoch 027 Train Loss: 0.2277 | Val Loss: 0.2280\n",
      "GCN | Epoch 028 Train Loss: 0.2273 | Val Loss: 0.2251\n",
      "GCN | Epoch 029 Train Loss: 0.2262 | Val Loss: 0.2224\n",
      "GCN | Epoch 030 Train Loss: 0.2269 | Val Loss: 0.2211\n",
      "GCN | Epoch 031 Train Loss: 0.2259 | Val Loss: 0.2221\n",
      "GCN | Epoch 032 Train Loss: 0.2248 | Val Loss: 0.2298\n",
      "GCN | Epoch 033 Train Loss: 0.2276 | Val Loss: 0.2244\n",
      "GCN | Epoch 034 Train Loss: 0.2260 | Val Loss: 0.2211\n",
      "GCN | Epoch 035 Train Loss: 0.2244 | Val Loss: 0.2208\n",
      "GCN | Epoch 036 Train Loss: 0.2244 | Val Loss: 0.2203\n",
      "GCN | Epoch 037 Train Loss: 0.2246 | Val Loss: 0.2201\n",
      "GCN | Epoch 038 Train Loss: 0.2242 | Val Loss: 0.2236\n",
      "GCN | Epoch 039 Train Loss: 0.2239 | Val Loss: 0.2211\n",
      "GCN | Epoch 040 Train Loss: 0.2234 | Val Loss: 0.2186\n",
      "GCN | Epoch 041 Train Loss: 0.2227 | Val Loss: 0.2217\n",
      "GCN | Epoch 042 Train Loss: 0.2231 | Val Loss: 0.2175\n",
      "GCN | Epoch 043 Train Loss: 0.2226 | Val Loss: 0.2180\n",
      "GCN | Epoch 044 Train Loss: 0.2220 | Val Loss: 0.2175\n",
      "GCN | Epoch 045 Train Loss: 0.2223 | Val Loss: 0.2197\n",
      "GCN | Epoch 046 Train Loss: 0.2229 | Val Loss: 0.2192\n",
      "GCN | Epoch 047 Train Loss: 0.2209 | Val Loss: 0.2164\n",
      "GCN | Epoch 048 Train Loss: 0.2215 | Val Loss: 0.2155\n",
      "GCN | Epoch 049 Train Loss: 0.2207 | Val Loss: 0.2162\n",
      "GCN | Epoch 050 Train Loss: 0.2198 | Val Loss: 0.2173\n",
      "GCN Test Loss: 0.2325\n",
      "Training GAT\n",
      "GAT | Epoch 001 Train Loss: 0.3425 | Val Loss: 0.2484\n",
      "GAT | Epoch 002 Train Loss: 0.2509 | Val Loss: 0.2448\n",
      "GAT | Epoch 003 Train Loss: 0.2466 | Val Loss: 0.2369\n",
      "GAT | Epoch 004 Train Loss: 0.2433 | Val Loss: 0.2366\n",
      "GAT | Epoch 005 Train Loss: 0.2413 | Val Loss: 0.2350\n",
      "GAT | Epoch 006 Train Loss: 0.2392 | Val Loss: 0.2350\n",
      "GAT | Epoch 007 Train Loss: 0.2385 | Val Loss: 0.2326\n",
      "GAT | Epoch 008 Train Loss: 0.2340 | Val Loss: 0.2287\n",
      "GAT | Epoch 009 Train Loss: 0.2317 | Val Loss: 0.2249\n",
      "GAT | Epoch 010 Train Loss: 0.2316 | Val Loss: 0.2282\n",
      "GAT | Epoch 011 Train Loss: 0.2291 | Val Loss: 0.2229\n",
      "GAT | Epoch 012 Train Loss: 0.2287 | Val Loss: 0.2234\n",
      "GAT | Epoch 013 Train Loss: 0.2280 | Val Loss: 0.2262\n",
      "GAT | Epoch 014 Train Loss: 0.2279 | Val Loss: 0.2236\n",
      "GAT | Epoch 015 Train Loss: 0.2268 | Val Loss: 0.2213\n",
      "GAT | Epoch 016 Train Loss: 0.2250 | Val Loss: 0.2222\n",
      "GAT | Epoch 017 Train Loss: 0.2257 | Val Loss: 0.2221\n",
      "GAT | Epoch 018 Train Loss: 0.2254 | Val Loss: 0.2236\n",
      "GAT | Epoch 019 Train Loss: 0.2259 | Val Loss: 0.2220\n",
      "GAT | Epoch 020 Train Loss: 0.2270 | Val Loss: 0.2242\n",
      "GAT | Epoch 021 Train Loss: 0.2241 | Val Loss: 0.2212\n",
      "GAT | Epoch 022 Train Loss: 0.2253 | Val Loss: 0.2215\n",
      "GAT | Epoch 023 Train Loss: 0.2248 | Val Loss: 0.2197\n",
      "GAT | Epoch 024 Train Loss: 0.2239 | Val Loss: 0.2210\n",
      "GAT | Epoch 025 Train Loss: 0.2245 | Val Loss: 0.2195\n",
      "GAT | Epoch 026 Train Loss: 0.2236 | Val Loss: 0.2246\n",
      "GAT | Epoch 027 Train Loss: 0.2241 | Val Loss: 0.2179\n",
      "GAT | Epoch 028 Train Loss: 0.2217 | Val Loss: 0.2188\n",
      "GAT | Epoch 029 Train Loss: 0.2224 | Val Loss: 0.2165\n",
      "GAT | Epoch 030 Train Loss: 0.2229 | Val Loss: 0.2180\n",
      "GAT | Epoch 031 Train Loss: 0.2219 | Val Loss: 0.2162\n",
      "GAT | Epoch 032 Train Loss: 0.2229 | Val Loss: 0.2172\n",
      "GAT | Epoch 033 Train Loss: 0.2211 | Val Loss: 0.2166\n",
      "GAT | Epoch 034 Train Loss: 0.2220 | Val Loss: 0.2162\n",
      "GAT | Epoch 035 Train Loss: 0.2211 | Val Loss: 0.2150\n",
      "GAT | Epoch 036 Train Loss: 0.2195 | Val Loss: 0.2224\n",
      "GAT | Epoch 037 Train Loss: 0.2206 | Val Loss: 0.2161\n",
      "GAT | Epoch 038 Train Loss: 0.2203 | Val Loss: 0.2147\n",
      "GAT | Epoch 039 Train Loss: 0.2196 | Val Loss: 0.2141\n",
      "GAT | Epoch 040 Train Loss: 0.2188 | Val Loss: 0.2142\n",
      "GAT | Epoch 041 Train Loss: 0.2200 | Val Loss: 0.2169\n",
      "GAT | Epoch 042 Train Loss: 0.2185 | Val Loss: 0.2156\n",
      "GAT | Epoch 043 Train Loss: 0.2180 | Val Loss: 0.2169\n",
      "GAT | Epoch 044 Train Loss: 0.2178 | Val Loss: 0.2166\n",
      "GAT | Epoch 045 Train Loss: 0.2174 | Val Loss: 0.2164\n",
      "GAT | Epoch 046 Train Loss: 0.2205 | Val Loss: 0.2137\n",
      "GAT | Epoch 047 Train Loss: 0.2172 | Val Loss: 0.2155\n",
      "GAT | Epoch 048 Train Loss: 0.2173 | Val Loss: 0.2158\n",
      "GAT | Epoch 049 Train Loss: 0.2180 | Val Loss: 0.2134\n",
      "GAT | Epoch 050 Train Loss: 0.2163 | Val Loss: 0.2125\n",
      "GAT Test Loss: 0.2284\n",
      "Summary of training:\n",
      "Model | Val Loss | Test Loss\n",
      "GCN  | 0.2155 | 0.2325\n",
      "GAT  | 0.2125 | 0.2284\n"
     ]
    }
   ],
   "source": [
    "sample_graph = train_dataset[0]\n",
    "in_channels = sample_graph.x.shape[1]\n",
    "out_channels = sample_graph.y.shape[1]\n",
    "results = {}\n",
    "results_rows = []\n",
    "roc_rows = []\n",
    "\n",
    "for model_type in ['GCN', 'GAT']:\n",
    "    print(f\"Training {model_type}\")\n",
    "    model = GNNModel(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=CONFIG['hidden_channels'],\n",
    "        out_channels=out_channels,\n",
    "        model_type=model_type,\n",
    "        num_layers=CONFIG['num_layers'],\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(CONFIG['device'])\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, CONFIG['epochs']+1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, CONFIG['device'])\n",
    "        val_loss = evaluate(model, val_loader, CONFIG['device'])\n",
    "\n",
    "        results_rows.append({\n",
    "            \"Model\": model_type,\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train Loss\": round(train_loss, 4),\n",
    "            \"Val Loss\": round(val_loss, 4)\n",
    "        })\n",
    "\n",
    "        print(f\"{model_type} | Epoch {epoch:03d} Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"best_{model_type}.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"Early stopping {model_type}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"best_{model_type}.pt\"))\n",
    "    test_loss = evaluate(model, test_loader, CONFIG['device'])\n",
    "    print(f\"{model_type} Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    results_rows.append({\n",
    "        \"Model\": model_type,\n",
    "        \"Epoch\": \"Best\",\n",
    "        \"Train Loss\": None,\n",
    "        \"Val Loss\": round(test_loss, 4),\n",
    "    })\n",
    "\n",
    "    results[model_type] = {'val_loss': best_val_loss, 'test_loss': test_loss}\n",
    "    \n",
    "\n",
    "print(\"Summary of training:\")\n",
    "print(\"Model | Val Loss | Test Loss\")\n",
    "for model_type, res in results.items():\n",
    "    print(f\"{model_type:4s} | {res['val_loss']:.4f} | {res['test_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79480020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scrollable_df(df, height):\n",
    "    html = df.to_html()\n",
    "    display(HTML(\n",
    "        f'<div style=\"max-height:{height}px; overflow-y:auto; border:1px solid #ddd; padding:10px;\">{html}</div>'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ffb26e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:500px; overflow-y:auto; border:1px solid #ddd; padding:10px;\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Val Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3078</td>\n",
       "      <td>0.2517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.2442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2483</td>\n",
       "      <td>0.2407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GCN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2445</td>\n",
       "      <td>0.2358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GCN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2423</td>\n",
       "      <td>0.2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GCN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2399</td>\n",
       "      <td>0.2331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GCN</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>0.2325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GCN</td>\n",
       "      <td>8</td>\n",
       "      <td>0.2368</td>\n",
       "      <td>0.2290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GCN</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.2292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GCN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>0.2322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GCN</td>\n",
       "      <td>11</td>\n",
       "      <td>0.2338</td>\n",
       "      <td>0.2270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GCN</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2316</td>\n",
       "      <td>0.2264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GCN</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2315</td>\n",
       "      <td>0.2264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GCN</td>\n",
       "      <td>14</td>\n",
       "      <td>0.2312</td>\n",
       "      <td>0.2266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GCN</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2316</td>\n",
       "      <td>0.2258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GCN</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2312</td>\n",
       "      <td>0.2277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GCN</td>\n",
       "      <td>17</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.2275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GCN</td>\n",
       "      <td>18</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>0.2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GCN</td>\n",
       "      <td>19</td>\n",
       "      <td>0.2298</td>\n",
       "      <td>0.2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GCN</td>\n",
       "      <td>20</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>0.2254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GCN</td>\n",
       "      <td>21</td>\n",
       "      <td>0.2289</td>\n",
       "      <td>0.2257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GCN</td>\n",
       "      <td>22</td>\n",
       "      <td>0.2289</td>\n",
       "      <td>0.2295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GCN</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.2255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GCN</td>\n",
       "      <td>24</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GCN</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>0.2282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GCN</td>\n",
       "      <td>26</td>\n",
       "      <td>0.2272</td>\n",
       "      <td>0.2227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GCN</td>\n",
       "      <td>27</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>0.2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GCN</td>\n",
       "      <td>28</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GCN</td>\n",
       "      <td>29</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.2224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GCN</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2269</td>\n",
       "      <td>0.2211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>GCN</td>\n",
       "      <td>31</td>\n",
       "      <td>0.2259</td>\n",
       "      <td>0.2221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>GCN</td>\n",
       "      <td>32</td>\n",
       "      <td>0.2248</td>\n",
       "      <td>0.2298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GCN</td>\n",
       "      <td>33</td>\n",
       "      <td>0.2276</td>\n",
       "      <td>0.2244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GCN</td>\n",
       "      <td>34</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.2211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GCN</td>\n",
       "      <td>35</td>\n",
       "      <td>0.2244</td>\n",
       "      <td>0.2208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>GCN</td>\n",
       "      <td>36</td>\n",
       "      <td>0.2244</td>\n",
       "      <td>0.2203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>GCN</td>\n",
       "      <td>37</td>\n",
       "      <td>0.2246</td>\n",
       "      <td>0.2201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>GCN</td>\n",
       "      <td>38</td>\n",
       "      <td>0.2242</td>\n",
       "      <td>0.2236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GCN</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2239</td>\n",
       "      <td>0.2211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>GCN</td>\n",
       "      <td>40</td>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>GCN</td>\n",
       "      <td>41</td>\n",
       "      <td>0.2227</td>\n",
       "      <td>0.2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>GCN</td>\n",
       "      <td>42</td>\n",
       "      <td>0.2231</td>\n",
       "      <td>0.2175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>GCN</td>\n",
       "      <td>43</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.2180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>GCN</td>\n",
       "      <td>44</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.2175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>GCN</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>0.2197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>GCN</td>\n",
       "      <td>46</td>\n",
       "      <td>0.2229</td>\n",
       "      <td>0.2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>GCN</td>\n",
       "      <td>47</td>\n",
       "      <td>0.2209</td>\n",
       "      <td>0.2164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>GCN</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2215</td>\n",
       "      <td>0.2155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>GCN</td>\n",
       "      <td>49</td>\n",
       "      <td>0.2207</td>\n",
       "      <td>0.2162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>GCN</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2198</td>\n",
       "      <td>0.2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>GCN</td>\n",
       "      <td>Best</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>GAT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3425</td>\n",
       "      <td>0.2484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>GAT</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2509</td>\n",
       "      <td>0.2448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>GAT</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2466</td>\n",
       "      <td>0.2369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>GAT</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>0.2366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>GAT</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.2350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>GAT</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2392</td>\n",
       "      <td>0.2350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>GAT</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2385</td>\n",
       "      <td>0.2326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GAT</td>\n",
       "      <td>8</td>\n",
       "      <td>0.2340</td>\n",
       "      <td>0.2287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>GAT</td>\n",
       "      <td>9</td>\n",
       "      <td>0.2317</td>\n",
       "      <td>0.2249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>GAT</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2316</td>\n",
       "      <td>0.2282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>GAT</td>\n",
       "      <td>11</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>0.2229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>GAT</td>\n",
       "      <td>12</td>\n",
       "      <td>0.2287</td>\n",
       "      <td>0.2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>GAT</td>\n",
       "      <td>13</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>GAT</td>\n",
       "      <td>14</td>\n",
       "      <td>0.2279</td>\n",
       "      <td>0.2236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>GAT</td>\n",
       "      <td>15</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.2213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>GAT</td>\n",
       "      <td>16</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.2222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>GAT</td>\n",
       "      <td>17</td>\n",
       "      <td>0.2257</td>\n",
       "      <td>0.2221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>GAT</td>\n",
       "      <td>18</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.2236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>GAT</td>\n",
       "      <td>19</td>\n",
       "      <td>0.2259</td>\n",
       "      <td>0.2220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>GAT</td>\n",
       "      <td>20</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.2242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>GAT</td>\n",
       "      <td>21</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>0.2212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>GAT</td>\n",
       "      <td>22</td>\n",
       "      <td>0.2253</td>\n",
       "      <td>0.2215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>GAT</td>\n",
       "      <td>23</td>\n",
       "      <td>0.2248</td>\n",
       "      <td>0.2197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>GAT</td>\n",
       "      <td>24</td>\n",
       "      <td>0.2239</td>\n",
       "      <td>0.2210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>GAT</td>\n",
       "      <td>25</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.2195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>GAT</td>\n",
       "      <td>26</td>\n",
       "      <td>0.2236</td>\n",
       "      <td>0.2246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>GAT</td>\n",
       "      <td>27</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>0.2179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>GAT</td>\n",
       "      <td>28</td>\n",
       "      <td>0.2217</td>\n",
       "      <td>0.2188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>GAT</td>\n",
       "      <td>29</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>0.2165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>GAT</td>\n",
       "      <td>30</td>\n",
       "      <td>0.2229</td>\n",
       "      <td>0.2180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>GAT</td>\n",
       "      <td>31</td>\n",
       "      <td>0.2219</td>\n",
       "      <td>0.2162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>GAT</td>\n",
       "      <td>32</td>\n",
       "      <td>0.2229</td>\n",
       "      <td>0.2172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>GAT</td>\n",
       "      <td>33</td>\n",
       "      <td>0.2211</td>\n",
       "      <td>0.2166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>GAT</td>\n",
       "      <td>34</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.2162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>GAT</td>\n",
       "      <td>35</td>\n",
       "      <td>0.2211</td>\n",
       "      <td>0.2150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>GAT</td>\n",
       "      <td>36</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.2224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>GAT</td>\n",
       "      <td>37</td>\n",
       "      <td>0.2206</td>\n",
       "      <td>0.2161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>GAT</td>\n",
       "      <td>38</td>\n",
       "      <td>0.2203</td>\n",
       "      <td>0.2147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>GAT</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.2141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>GAT</td>\n",
       "      <td>40</td>\n",
       "      <td>0.2188</td>\n",
       "      <td>0.2142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>GAT</td>\n",
       "      <td>41</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>GAT</td>\n",
       "      <td>42</td>\n",
       "      <td>0.2185</td>\n",
       "      <td>0.2156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>GAT</td>\n",
       "      <td>43</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>GAT</td>\n",
       "      <td>44</td>\n",
       "      <td>0.2178</td>\n",
       "      <td>0.2166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>GAT</td>\n",
       "      <td>45</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>GAT</td>\n",
       "      <td>46</td>\n",
       "      <td>0.2205</td>\n",
       "      <td>0.2137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>GAT</td>\n",
       "      <td>47</td>\n",
       "      <td>0.2172</td>\n",
       "      <td>0.2155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>GAT</td>\n",
       "      <td>48</td>\n",
       "      <td>0.2173</td>\n",
       "      <td>0.2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>GAT</td>\n",
       "      <td>49</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.2134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>GAT</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2163</td>\n",
       "      <td>0.2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>GAT</td>\n",
       "      <td>Best</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_rows)\n",
    "display_scrollable_df(results_df, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af850dd9",
   "metadata": {},
   "source": [
    "AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d09796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_task_auc(model, loader, device):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data.x = data.x.to(torch.float)\n",
    "            data.edge_index = data.edge_index.to(torch.long)\n",
    "            if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.to(torch.float)\n",
    "            data.y = data.y.to(torch.float)\n",
    "\n",
    "            data = data.to(device)\n",
    "\n",
    "            logits = model(data.x, data.edge_index, data.batch)\n",
    "            labels = data.y\n",
    "\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    logits = torch.cat(all_logits, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    logits = logits.numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    aucs = []\n",
    "    for task_idx in range(labels.shape[1]):\n",
    "        valid = ~np.isnan(labels[:, task_idx])\n",
    "        if valid.sum() > 0:\n",
    "            aucs.append(roc_auc_score(labels[valid, task_idx], logits[valid, task_idx]))\n",
    "        else:\n",
    "            aucs.append(np.nan)\n",
    "\n",
    "    return aucs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8564d98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:500px; overflow-y:auto; border:1px solid #ddd; padding:10px;\"><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>GCN AUC-ROC</th>\n",
       "      <th>GAT AUC-ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NR-AR</td>\n",
       "      <td>0.768311</td>\n",
       "      <td>0.744717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NR-AR-LBD</td>\n",
       "      <td>0.858210</td>\n",
       "      <td>0.869908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NR-AhR</td>\n",
       "      <td>0.821553</td>\n",
       "      <td>0.829204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NR-Aromatase</td>\n",
       "      <td>0.780216</td>\n",
       "      <td>0.727327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NR-ER</td>\n",
       "      <td>0.639675</td>\n",
       "      <td>0.675570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NR-ER-LBD</td>\n",
       "      <td>0.758397</td>\n",
       "      <td>0.819998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NR-PPAR-gamma</td>\n",
       "      <td>0.811833</td>\n",
       "      <td>0.830002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SR-ARE</td>\n",
       "      <td>0.681930</td>\n",
       "      <td>0.684876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SR-ATAD5</td>\n",
       "      <td>0.722981</td>\n",
       "      <td>0.767969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SR-HSE</td>\n",
       "      <td>0.705305</td>\n",
       "      <td>0.769661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SR-MMP</td>\n",
       "      <td>0.804613</td>\n",
       "      <td>0.780619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SR-p53</td>\n",
       "      <td>0.762788</td>\n",
       "      <td>0.773072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_names = data_train['labels_cols']\n",
    "\n",
    "gcn_model = GNNModel(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=CONFIG['hidden_channels'],\n",
    "    out_channels=out_channels,\n",
    "    model_type='GCN',\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(CONFIG['device'])\n",
    "gcn_model.load_state_dict(torch.load(\"best_GCN.pt\"))\n",
    "\n",
    "gat_model = GNNModel(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=CONFIG['hidden_channels'],\n",
    "    out_channels=out_channels,\n",
    "    model_type='GAT',\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(CONFIG['device'])\n",
    "gat_model.load_state_dict(torch.load(\"best_GAT.pt\"))\n",
    "\n",
    "gcn_auc = compute_task_auc(gcn_model, test_loader, CONFIG['device'])\n",
    "gat_auc = compute_task_auc(gat_model, test_loader, CONFIG['device'])\n",
    "\n",
    "roc_df = pd.DataFrame({\n",
    "    'Task': task_names,\n",
    "    'GCN AUC-ROC': gcn_auc,\n",
    "    'GAT AUC-ROC': gat_auc\n",
    "})\n",
    "\n",
    "display_scrollable_df(roc_df, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9945fa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1173, 9])\n",
      "y: torch.Size([64, 12])\n",
      "batch: torch.Size([1173])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_loader))\n",
    "print(\"x:\", batch.x.shape)\n",
    "print(\"y:\", batch.y.shape)\n",
    "print(\"batch:\", batch.batch.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
